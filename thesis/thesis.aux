\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Intriguing Properties of Neural Networks}{1}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces By means of a noisy perturbation of the image, an image classifier model can be fooled into believing with high confidence that what was correclty labeled as a dog is now a cat.\relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:aecatdog}{{1.1}{2}{By means of a noisy perturbation of the image, an image classifier model can be fooled into believing with high confidence that what was correclty labeled as a dog is now a cat.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Activation Functions and Robustness}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure of the Thesis}{3}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Fundamentals}{5}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Neural Networks}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definition}{7}{section.2.1}\protected@file@percent }
\newlabel{layer}{{2.2}{7}{Definition}{equation.2.1.2}{}}
\newlabel{step}{{2.4}{8}{Definition}{equation.2.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Graphic representation of a one layer NN also know as MLP(ref. )\relax }}{8}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{8}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Training}{8}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A Feedforward Neural Network with 3 hidden layers(ref. Michael A. Nielsen, Neural Networks and Deep Learning, Determination Press', 2015)\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:fnn}{{2.2}{9}{A Feedforward Neural Network with 3 hidden layers(ref. Michael A. Nielsen, Neural Networks and Deep Learning, Determination Press', 2015)\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Loss Function}{9}{subsection.2.2.1}\protected@file@percent }
\newlabel{erm}{{2.5}{9}{Loss Function}{equation.2.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Gradient Descent}{9}{subsection.2.2.2}\protected@file@percent }
\newlabel{bp1}{{2.7}{10}{Gradient Descent}{equation.2.2.7}{}}
\newlabel{bp2}{{2.8}{10}{Gradient Descent}{equation.2.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A 1-D representation of 4 gradient descent steps\relax }}{11}{figure.caption.6}\protected@file@percent }
\newlabel{fig:gd}{{2.3}{11}{A 1-D representation of 4 gradient descent steps\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Convex and Non-convex optimization landscapes\relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{fig:convnonconv}{{2.4}{12}{Convex and Non-convex optimization landscapes\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualization of global, local and saddle points. How can A reach the global minimum?\relax }}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:globlocsad}{{2.5}{12}{Visualization of global, local and saddle points. How can A reach the global minimum?\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{12}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Plot of the Sigmoid function and its first derivative.\relax }}{13}{figure.caption.9}\protected@file@percent }
\newlabel{fig:sigmoid}{{2.6}{13}{Plot of the Sigmoid function and its first derivative.\relax }{figure.caption.9}{}}
\newlabel{elu}{{2.18}{14}{Activation Functions}{equation.2.3.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces ELU, LeakyReLU($\alpha =0.1$) and Swish functions plotted together. It can be seen that they mostly differ for negative values whereas behaving very similar to ReLU for positive arguments.\relax }}{14}{figure.caption.10}\protected@file@percent }
\newlabel{fig:afuncs}{{2.7}{14}{ELU, LeakyReLU($\alpha =0.1$) and Swish functions plotted together. It can be seen that they mostly differ for negative values whereas behaving very similar to ReLU for positive arguments.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}CNNs: Convolutional Neural Networks}{15}{section.2.4}\protected@file@percent }
\newlabel{cnns}{{2.4}{15}{CNNs: Convolutional Neural Networks}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A convolution that produces the first element of the activation map for the given filter.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:conv}{{2.8}{16}{A convolution that produces the first element of the activation map for the given filter.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Max (left) and Average (right) pooling layer with 2x2 window size.\relax }}{16}{figure.caption.12}\protected@file@percent }
\newlabel{fig:pooling}{{2.9}{16}{Max (left) and Average (right) pooling layer with 2x2 window size.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces General Architecture of CNN for ImageNet (Ref. ).\relax }}{17}{figure.caption.13}\protected@file@percent }
\newlabel{fig:cnnarch}{{2.10}{17}{General Architecture of CNN for ImageNet (Ref. ).\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}From Neural Networks to Deep Neural Networks}{17}{section.2.5}\protected@file@percent }
\newlabel{dnns}{{2.5}{17}{From Neural Networks to Deep Neural Networks}{section.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Inception Block overview, $1\times 1$ convolutions are used to reduce the number of filters lowering complexity. Source: Dive into Deep Learning, chapt. 7.4\relax }}{18}{figure.caption.14}\protected@file@percent }
\newlabel{fig:inception}{{2.11}{18}{Inception Block overview, $1\times 1$ convolutions are used to reduce the number of filters lowering complexity. Source: Dive into Deep Learning, chapt. 7.4\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Source: Deep Residual Learning for Image Recognition\relax }}{19}{figure.caption.15}\protected@file@percent }
\newlabel{fig:degradation}{{2.12}{19}{Source: Deep Residual Learning for Image Recognition\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Source: Deep Residual Learning for Image Recognition\relax }}{19}{figure.caption.16}\protected@file@percent }
\newlabel{fig:residual}{{2.13}{19}{Source: Deep Residual Learning for Image Recognition\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Source: Dive Deep into Deep Learning\relax }}{20}{figure.caption.17}\protected@file@percent }
\newlabel{fig:resnet}{{2.14}{20}{Source: Dive Deep into Deep Learning\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Adversarial Examples Theory}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Another Optimization problem}{22}{section.3.1}\protected@file@percent }
\newlabel{attackobj}{{3.2}{22}{Another Optimization problem}{equation.3.1.2}{}}
\newlabel{linfnorm}{{3.3}{23}{Another Optimization problem}{equation.3.1.3}{}}
\newlabel{targetedattack}{{3.5}{23}{Another Optimization problem}{equation.3.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Fast Gradient Sign Method}{23}{subsection.3.1.1}\protected@file@percent }
\newlabel{fgsm}{{3.1.1}{23}{Fast Gradient Sign Method}{subsection.3.1.1}{}}
\newlabel{pgd}{{3.6}{23}{Fast Gradient Sign Method}{equation.3.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Projected Gradient Descent}{24}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}White, Grey and Black Box Attacks}{24}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Defenses}{25}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Detection Methods}{25}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Robust Optimization and Adversarial Training}{26}{subsection.3.2.2}\protected@file@percent }
\newlabel{robopt}{{3.9}{26}{Robust Optimization and Adversarial Training}{equation.3.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Metzen \textit  {et al.} (Ref. ) used a ResNet as original classifier, plus dections is made thanks to many interleaving detectors between each residual block. Each detector is implemented as a DNN that learns how to spot the presence of an attack by looking at the activations layer's distributions during training.\relax }}{26}{figure.caption.18}\protected@file@percent }
\newlabel{fig:detection}{{3.1}{26}{Metzen \textit {et al.} (Ref. ) used a ResNet as original classifier, plus dections is made thanks to many interleaving detectors between each residual block. Each detector is implemented as a DNN that learns how to spot the presence of an attack by looking at the activations layer's distributions during training.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Provable Robustness}{27}{subsection.3.2.3}\protected@file@percent }
\newlabel{provrob}{{3.2.3}{27}{Provable Robustness}{subsection.3.2.3}{}}
\newlabel{LC}{{3.12}{28}{Provable Robustness}{equation.3.2.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Non-Parametric Activation Functions}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{srelu}{{4.1}{29}{Non-Parametric Activation Functions}{equation.4.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Adaptive Piece-Wise Linear Activation Functions}{30}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Spline Activation Functions}{30}{section.4.2}\protected@file@percent }
\newlabel{saf}{{4.4}{30}{Spline Activation Functions}{equation.4.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Maxout Functions}{31}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A maxout function with a one-dimensional input and K = 3. The three linear dot products are shown with light gray, while the max and thus resulting activation is shown in shaded red. Source: (Ref. )\relax }}{31}{figure.caption.19}\protected@file@percent }
\newlabel{fig:maxout}{{4.1}{31}{A maxout function with a one-dimensional input and K = 3. The three linear dot products are shown with light gray, while the max and thus resulting activation is shown in shaded red. Source: (Ref. )\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Kernel-Based Activation Functions}{31}{section.4.4}\protected@file@percent }
\newlabel{gauskaf}{{4.8}{32}{Kernel-Based Activation Functions}{equation.4.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The shape of a KAF with randomly initialized coefficients is shown in light-starred orange. The final learned shape of the same KAF after training is shown with a continuous orange line. Notice the strong difference between the two functions and how the final one resembles a shifted tanh.\relax }}{33}{figure.caption.20}\protected@file@percent }
\newlabel{fig:rand}{{4.2}{33}{The shape of a KAF with randomly initialized coefficients is shown in light-starred orange. The final learned shape of the same KAF after training is shown with a continuous orange line. Notice the strong difference between the two functions and how the final one resembles a shifted tanh.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The shape of a KAF with coefficients initialized through ridge regression to approximate the ELU function is shown in light-starred blue. The final shape after training is shown with a continuous blue line. Notice how the difference between the two is less evident comparing to randomly initialized KAF (left). Both KAFs (left and right) were trained on the same neuron for the \textit  {MNIST} dataset using 5 epochs and same overall settings. \relax }}{33}{figure.caption.20}\protected@file@percent }
\newlabel{fig:ridge}{{4.3}{33}{The shape of a KAF with coefficients initialized through ridge regression to approximate the ELU function is shown in light-starred blue. The final shape after training is shown with a continuous blue line. Notice how the difference between the two is less evident comparing to randomly initialized KAF (left). Both KAFs (left and right) were trained on the same neuron for the \textit {MNIST} dataset using 5 epochs and same overall settings. \relax }{figure.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Each row summarize the results obtained using different activation functions on a specific task. Underlined entries stand for best result achieved. $^*$: feedforward neural networks with 3 hidden layers of 100 neurons are used for fixed or parametric activation functions whilst a single hidden layer is used for KAF activation function. $^{**}$: feedforward neural networks with 5 hidden layers and 300 neurons each for fixed or parametric activation functions whilst 2 hidden layers with the same number of neurons for non-parametric activation functions   $^{***}$: CNNs made by stacking 5 convolutional blocks, each composed by (a) a convolutive layer with 150 filters, with a filter size of 5 \IeC {\texttimes } 5 and a stride of 1; (b) a max-pooling operation over 3 \IeC {\texttimes } 3 windows with stride of 2; (c) a dropout layer with probability of 0.25. See the original article for a full description of the architectures, hyperparams and training settings.\relax }}{34}{table.caption.21}\protected@file@percent }
\newlabel{tab:scardapanekafexp}{{4.1}{34}{Each row summarize the results obtained using different activation functions on a specific task. Underlined entries stand for best result achieved. $^*$: feedforward neural networks with 3 hidden layers of 100 neurons are used for fixed or parametric activation functions whilst a single hidden layer is used for KAF activation function. $^{**}$: feedforward neural networks with 5 hidden layers and 300 neurons each for fixed or parametric activation functions whilst 2 hidden layers with the same number of neurons for non-parametric activation functions \\ $^{***}$: CNNs made by stacking 5 convolutional blocks, each composed by (a) a convolutive layer with 150 filters, with a filter size of 5 × 5 and a stride of 1; (b) a max-pooling operation over 3 × 3 windows with stride of 2; (c) a dropout layer with probability of 0.25. See the original article for a full description of the architectures, hyperparams and training settings.\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Robustness of Kafnets}{35}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Related Works}{37}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}K-Winners Take All}{37}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A plot of a 1-dimensional k-WTA interpolating a curve. As stressed by the figure, k-WTAs are piece-wise linar functions where points of non-differentiability are very much densely distributed. Any small change in the input results in an abrupt change in the function's value. Source: (Ref. )\relax }}{38}{figure.caption.22}\protected@file@percent }
\newlabel{fig:kwta}{{5.1}{38}{A plot of a 1-dimensional k-WTA interpolating a curve. As stressed by the figure, k-WTAs are piece-wise linar functions where points of non-differentiability are very much densely distributed. Any small change in the input results in an abrupt change in the function's value. Source: (Ref. )\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Smooth Adversarial Training}{38}{section.5.2}\protected@file@percent }
\newlabel{SAT}{{5.2}{38}{Smooth Adversarial Training}{section.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Plot of the first derivative for ReLU, ELU and Swish activations. Except for the rectified linear unit, both the ELU and Swish have continuous derivative.\relax }}{39}{figure.caption.23}\protected@file@percent }
\newlabel{fig:smoothaf}{{5.2}{39}{Plot of the first derivative for ReLU, ELU and Swish activations. Except for the rectified linear unit, both the ELU and Swish have continuous derivative.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Compared to ReLU, all smooth activation functions showed a significant increase in robustness while some even improved accuracy. In the original work, 2 others smooth activation functions were used, namely SmoothReLU and Softplus. We did not include them since they are not commonly used in practice. Source: (Ref. Adversarlial Smoothness)\relax }}{40}{figure.caption.24}\protected@file@percent }
\newlabel{fig:smoothresults}{{5.3}{40}{Compared to ReLU, all smooth activation functions showed a significant increase in robustness while some even improved accuracy. In the original work, 2 others smooth activation functions were used, namely SmoothReLU and Softplus. We did not include them since they are not commonly used in practice. Source: (Ref. Adversarlial Smoothness)\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Solution Approach}{41}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{swenv}{{6}{41}{Solution Approach}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}KAFs May Be Good Candidates}{41}{section.6.1}\protected@file@percent }
\newlabel{fig:actdist11}{{\caption@xref {fig:actdist11}{ on input line 1226}}{42}{KAFs May Be Good Candidates}{figure.caption.25}{}}
\newlabel{fig:actdist12}{{\caption@xref {fig:actdist12}{ on input line 1231}}{43}{KAFs May Be Good Candidates}{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Each histogram shows the distribution of the activation values for the first neuron of the first channel, for each layer. The distribution colored in violet represents the activations for the batch of perturbed samples while the light-green stands for the distribution of values the neuron gets when we feed the network with original samples. We also plot, when possible, the actual shape of the activation function acting on the neuron with a red line. The effects of the perturbation can be appreciated especially in the last layer where distributions differ the most.\relax }}{43}{figure.caption.27}\protected@file@percent }
\newlabel{fig:actdist13}{{6.1}{43}{Each histogram shows the distribution of the activation values for the first neuron of the first channel, for each layer. The distribution colored in violet represents the activations for the batch of perturbed samples while the light-green stands for the distribution of values the neuron gets when we feed the network with original samples. We also plot, when possible, the actual shape of the activation function acting on the neuron with a red line. The effects of the perturbation can be appreciated especially in the last layer where distributions differ the most.\relax }{figure.caption.27}{}}
\newlabel{fig:actdist21}{{\caption@xref {fig:actdist21}{ on input line 1254}}{44}{KAFs May Be Good Candidates}{figure.caption.28}{}}
\newlabel{fig:actdist22}{{\caption@xref {fig:actdist22}{ on input line 1259}}{44}{KAFs May Be Good Candidates}{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Each histogram shows the distribution of the mean value of the activations for a random layer's filter, in the case of a convolutive layer, or for every neurons in the layer, in the case of dense (last) layer. The effects of the perturbation are less visible when the net is equipped with KAF activation functions.\relax }}{45}{figure.caption.30}\protected@file@percent }
\newlabel{fig:actdist23}{{6.2}{45}{Each histogram shows the distribution of the mean value of the activations for a random layer's filter, in the case of a convolutive layer, or for every neurons in the layer, in the case of dense (last) layer. The effects of the perturbation are less visible when the net is equipped with KAF activation functions.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Fast is Better than Free Adversarial Training}{46}{section.6.2}\protected@file@percent }
\newlabel{fbf}{{6.2}{46}{Fast is Better than Free Adversarial Training}{section.6.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Evaluation}{49}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Learning rate evolution during a one-cyclic triangular learning rate schedule. Here the minimum value is 0.00005 and maximum value is 0.0005.\relax }}{50}{figure.caption.34}\protected@file@percent }
\newlabel{fig:oclr}{{7.1}{50}{Learning rate evolution during a one-cyclic triangular learning rate schedule. Here the minimum value is 0.00005 and maximum value is 0.0005.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Loss evolution plot with respect to exponentially increasing learning rates. The vertical dot-dashed red line denote the learning rate value where the loss has the steepest decrease. While the minimum value matches with the smallest learning rate, the maximum one should be picked around $10^{-1}$.\relax }}{50}{figure.caption.35}\protected@file@percent }
\newlabel{fig:lrf}{{7.2}{50}{Loss evolution plot with respect to exponentially increasing learning rates. The vertical dot-dashed red line denote the learning rate value where the loss has the steepest decrease. While the minimum value matches with the smallest learning rate, the maximum one should be picked around $10^{-1}$.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}VGG Inspired Architectures Results}{51}{section.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Visualization of the general structure of a VGG inspired CNN. During the experiments we will modify the activation function.\relax }}{52}{figure.caption.36}\protected@file@percent }
\newlabel{fig:vgg}{{7.3}{52}{Visualization of the general structure of a VGG inspired CNN. During the experiments we will modify the activation function.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Increasing the number of attack iterations inside PGD stops being effective after a certain threshold. With our set-up, 50 iterations are enough to craft an optimal attack. Moreover, notice how the Kafnet improves robustness over the baseline. \relax }}{52}{figure.caption.37}\protected@file@percent }
\newlabel{fig:pgditers}{{7.4}{52}{Increasing the number of attack iterations inside PGD stops being effective after a certain threshold. With our set-up, 50 iterations are enough to craft an optimal attack. Moreover, notice how the Kafnet improves robustness over the baseline. \relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Robustness/Accuracy visualization for a VGG-inspired architecture equipped with different activation functions. Fixed activation functions are shown with a thin diamond whereas non-parametric KAFs with a filled x. Swish achieves best robustness and KAFs generally tend to improve both robustness and accuracy with respect to a ReLU baseline.\relax }}{54}{figure.caption.38}\protected@file@percent }
\newlabel{fig:accrob1}{{7.5}{54}{Robustness/Accuracy visualization for a VGG-inspired architecture equipped with different activation functions. Fixed activation functions are shown with a thin diamond whereas non-parametric KAFs with a filled x. Swish achieves best robustness and KAFs generally tend to improve both robustness and accuracy with respect to a ReLU baseline.\relax }{figure.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Summary of the results gained from changing activation functions in an adversarially trained VGG network. Bold denotes best result while underline best trade-off.\relax }}{54}{table.caption.39}\protected@file@percent }
\newlabel{tab:vggrob}{{7.1}{54}{Summary of the results gained from changing activation functions in an adversarially trained VGG network. Bold denotes best result while underline best trade-off.\relax }{table.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Accuracy and loss evolution during AT. The orange denotes the metric evaluated on the train set while with blue we denote the metric for the test set. In the left the plot for KAF training is shown and in the right for KAF\_ELU. Notice how in both cases there is no sign of convergence at the 60-th epoch.\relax }}{54}{figure.caption.40}\protected@file@percent }
\newlabel{fig:vgghistory}{{7.6}{54}{Accuracy and loss evolution during AT. The orange denotes the metric evaluated on the train set while with blue we denote the metric for the test set. In the left the plot for KAF training is shown and in the right for KAF\_ELU. Notice how in both cases there is no sign of convergence at the 60-th epoch.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Exploding Gradients with KafNets}{55}{section.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces \relax }}{56}{figure.caption.41}\protected@file@percent }
\newlabel{fig:explgrad}{{7.7}{56}{\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces \relax }}{57}{figure.caption.42}\protected@file@percent }
\newlabel{fig:gradkaf1}{{7.8}{57}{\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces \relax }}{58}{figure.caption.43}\protected@file@percent }
\newlabel{fig:gradconv1}{{7.9}{58}{\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces \relax }}{59}{figure.caption.44}\protected@file@percent }
\newlabel{fig:gradkaf15}{{7.10}{59}{\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces \relax }}{59}{figure.caption.45}\protected@file@percent }
\newlabel{fig:gradconv49}{{7.11}{59}{\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}ResNet20 Inspired Architectures Results}{59}{section.7.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Summary of the robustness gained from changing activation functions in an adversarially trained ResNet20. Bold denotes best result while underline best trade-off.\relax }}{60}{table.caption.46}\protected@file@percent }
\newlabel{tab:resnetrob}{{7.2}{60}{Summary of the robustness gained from changing activation functions in an adversarially trained ResNet20. Bold denotes best result while underline best trade-off.\relax }{table.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Robustness/accuracy visualization for a ResNet20 equipped with different activation functions. Fixed activation functions are shown with a thin diamond whereas non-parametric KAFs with a filled x. Swish achieves best robustness and KAFs generally tend to improve both robustness and accuracy with respect to a ReLU baseline.\relax }}{60}{figure.caption.47}\protected@file@percent }
\newlabel{fig:rob_af2}{{7.12}{60}{Robustness/accuracy visualization for a ResNet20 equipped with different activation functions. Fixed activation functions are shown with a thin diamond whereas non-parametric KAFs with a filled x. Swish achieves best robustness and KAFs generally tend to improve both robustness and accuracy with respect to a ReLU baseline.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Conclusions and Future Works}{63}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Conclusions}{63}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Future Works}{64}{section.8.2}\protected@file@percent }
