\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Intriguing Properties of Neural Networks}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Smooth Activation Functions and Robustness}{1}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure of the Thesis}{1}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Fundamentals}{3}{part.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Neural Networks}{5}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Definition}{5}{section.2.1}\protected@file@percent }
\newlabel{layer}{{2.2}{5}{Definition}{equation.2.1.2}{}}
\newlabel{step}{{2.4}{6}{Definition}{equation.2.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Graphic representation of a one layer NN also know as MLP(ref. )\relax }}{6}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{6}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Training}{6}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A Feedforward Neural Network with 3 hidden layers(ref. Michael A. Nielsen, Neural Networks and Deep Learning, Determination Press', 2015)\relax }}{7}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fnn}{{2.2}{7}{A Feedforward Neural Network with 3 hidden layers(ref. Michael A. Nielsen, Neural Networks and Deep Learning, Determination Press', 2015)\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Loss Function}{7}{subsection.2.2.1}\protected@file@percent }
\newlabel{erm}{{2.5}{7}{Loss Function}{equation.2.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Gradient Descent}{7}{subsection.2.2.2}\protected@file@percent }
\newlabel{bp1}{{2.7}{8}{Gradient Descent}{equation.2.2.7}{}}
\newlabel{bp2}{{2.8}{8}{Gradient Descent}{equation.2.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A 1-D representation of 4 gradient descent steps\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:gd}{{2.3}{9}{A 1-D representation of 4 gradient descent steps\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Convex and Non-convex optimization landscapes\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:convnonconv}{{2.4}{10}{Convex and Non-convex optimization landscapes\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualization of global, local and saddle points. How can A reach the global minimum?\relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:globlocsad}{{2.5}{10}{Visualization of global, local and saddle points. How can A reach the global minimum?\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{10}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Plot of the Sigmoid function and its first derivative.\relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:sigmoid}{{2.6}{11}{Plot of the Sigmoid function and its first derivative.\relax }{figure.caption.8}{}}
\newlabel{elu}{{2.18}{12}{Activation Functions}{equation.2.3.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces ELU, LeakyReLU($\alpha =0.1$) and Swish functions plotted together. It can be seen that they mostly differ for negative values whereas behaving very similar to ReLU for positive arguments.\relax }}{12}{figure.caption.9}\protected@file@percent }
\newlabel{fig:afuncs}{{2.7}{12}{ELU, LeakyReLU($\alpha =0.1$) and Swish functions plotted together. It can be seen that they mostly differ for negative values whereas behaving very similar to ReLU for positive arguments.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}CNNs: Convolutional Neural Networks}{13}{section.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A convolution that produces the first element of the activation map for the given filter.\relax }}{14}{figure.caption.10}\protected@file@percent }
\newlabel{fig:conv}{{2.8}{14}{A convolution that produces the first element of the activation map for the given filter.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Max (left) and Average (right) pooling layer with 2x2 window size.\relax }}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:pooling}{{2.9}{14}{Max (left) and Average (right) pooling layer with 2x2 window size.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces General Architecture of CNN for ImageNet (Ref. ).\relax }}{15}{figure.caption.12}\protected@file@percent }
\newlabel{fig:cnnarch}{{2.10}{15}{General Architecture of CNN for ImageNet (Ref. ).\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}From Neural Networks to Deep Neural Networks}{15}{section.2.5}\protected@file@percent }
\newlabel{dnns}{{2.5}{15}{From Neural Networks to Deep Neural Networks}{section.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Inception Block overview, $1\times 1$ convolutions are used to reduce the number of filters lowering complexity. Source: Dive into Deep Learning, chapt. 7.4\relax }}{16}{figure.caption.13}\protected@file@percent }
\newlabel{fig:inception}{{2.11}{16}{Inception Block overview, $1\times 1$ convolutions are used to reduce the number of filters lowering complexity. Source: Dive into Deep Learning, chapt. 7.4\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Source: Deep Residual Learning for Image Recognition\relax }}{17}{figure.caption.14}\protected@file@percent }
\newlabel{fig:degradation}{{2.12}{17}{Source: Deep Residual Learning for Image Recognition\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Source: Deep Residual Learning for Image Recognition\relax }}{17}{figure.caption.15}\protected@file@percent }
\newlabel{fig:residual}{{2.13}{17}{Source: Deep Residual Learning for Image Recognition\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Source: Dive Deep into Deep Learning\relax }}{18}{figure.caption.16}\protected@file@percent }
\newlabel{fig:resnet}{{2.14}{18}{Source: Dive Deep into Deep Learning\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Adversarial Examples Theory}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Another Optimization problem}{20}{section.3.1}\protected@file@percent }
\newlabel{attackobj}{{3.2}{20}{Another Optimization problem}{equation.3.1.2}{}}
\newlabel{linfnorm}{{3.3}{21}{Another Optimization problem}{equation.3.1.3}{}}
\newlabel{targetedattack}{{3.5}{21}{Another Optimization problem}{equation.3.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Fast Gradient Sign Method}{21}{subsection.3.1.1}\protected@file@percent }
\newlabel{fgsm}{{3.1.1}{21}{Fast Gradient Sign Method}{subsection.3.1.1}{}}
\newlabel{pgd}{{3.6}{21}{Fast Gradient Sign Method}{equation.3.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Projected Gradient Descent}{22}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}White, Grey and Black Box Attacks}{22}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Defenses}{23}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Detection Methods}{23}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Robust Optimization}{24}{subsection.3.2.2}\protected@file@percent }
\newlabel{robopt}{{3.9}{24}{Robust Optimization}{equation.3.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Metzen \textit  {et al.} (Ref. ) used a ResNet as original classifier, plus dections is made thanks to many interleaving detectors between each residual block. Each detector is implemented as a DNN that learns how to spot the presence of an attack by looking at the activations layer's distributions during training.\relax }}{24}{figure.caption.17}\protected@file@percent }
\newlabel{fig:detection}{{3.1}{24}{Metzen \textit {et al.} (Ref. ) used a ResNet as original classifier, plus dections is made thanks to many interleaving detectors between each residual block. Each detector is implemented as a DNN that learns how to spot the presence of an attack by looking at the activations layer's distributions during training.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Provable Robustness}{25}{subsection.3.2.3}\protected@file@percent }
\newlabel{LC}{{3.12}{26}{Provable Robustness}{equation.3.2.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Non-Parametric Activation Functions}{27}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{srelu}{{4.1}{27}{Non-Parametric Activation Functions}{equation.4.0.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Adaptive Piece-Wise Linear Activation Functions}{28}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Spline Activation Functions}{28}{section.4.2}\protected@file@percent }
\newlabel{saf}{{4.4}{28}{Spline Activation Functions}{equation.4.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Maxout Functions}{29}{section.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A maxout function with a one-dimensional input and K = 3. The three linear dot products are shown with light gray, while the max and thus resulting activation is shown in shaded red. Source: (Ref. )\relax }}{29}{figure.caption.18}\protected@file@percent }
\newlabel{fig:maxout}{{4.1}{29}{A maxout function with a one-dimensional input and K = 3. The three linear dot products are shown with light gray, while the max and thus resulting activation is shown in shaded red. Source: (Ref. )\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Kernel-Based Activation Functions}{29}{section.4.4}\protected@file@percent }
\newlabel{gauskaf}{{4.8}{30}{Kernel-Based Activation Functions}{equation.4.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The shape of a KAF with randomly initialized coefficients is shown in light-starred orange. The final learned shape of the same KAF after training is shown with a continuous orange line. Notice the strong difference between the two functions and how the final one resembles a shifted tanh.\relax }}{31}{figure.caption.19}\protected@file@percent }
\newlabel{fig:rand}{{4.2}{31}{The shape of a KAF with randomly initialized coefficients is shown in light-starred orange. The final learned shape of the same KAF after training is shown with a continuous orange line. Notice the strong difference between the two functions and how the final one resembles a shifted tanh.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The shape of a KAF with coefficients initialized through ridge regression to approximate the ELU function is shown in light-starred blue. The final shape after training is shown with a continuous blue line. Notice how the difference between the two is less evident comparing to randomly initialized KAF (left). Both KAFs (left and right) were trained on the same neuron for the \textit  {MNIST} dataset using 5 epochs and same overall settings. \relax }}{31}{figure.caption.19}\protected@file@percent }
\newlabel{fig:ridge}{{4.3}{31}{The shape of a KAF with coefficients initialized through ridge regression to approximate the ELU function is shown in light-starred blue. The final shape after training is shown with a continuous blue line. Notice how the difference between the two is less evident comparing to randomly initialized KAF (left). Both KAFs (left and right) were trained on the same neuron for the \textit {MNIST} dataset using 5 epochs and same overall settings. \relax }{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Each row summarize the results obtained using different activation functions on a specific task. Underlined entries stand for best result achieved. $^*$: feedforward neural networks with 3 hidden layers of 100 neurons are used for fixed or parametric activation functions whilst a single hidden layer is used for KAF activation function. $^{**}$: feedforward neural networks with 5 hidden layers and 300 neurons each for fixed or parametric activation functions whilst 2 hidden layers with the same number of neurons for non-parametric activation functions   $^{***}$: CNNs made by stacking 5 convolutional blocks, each composed by (a) a convolutive layer with 150 filters, with a filter size of 5 \IeC {\texttimes } 5 and a stride of 1; (b) a max-pooling operation over 3 \IeC {\texttimes } 3 windows with stride of 2; (c) a dropout layer with probability of 0.25. See the original article for a full description of the architectures, hyperparams and training settings.\relax }}{32}{table.caption.20}\protected@file@percent }
\newlabel{tab:scardapanekafexp}{{4.1}{32}{Each row summarize the results obtained using different activation functions on a specific task. Underlined entries stand for best result achieved. $^*$: feedforward neural networks with 3 hidden layers of 100 neurons are used for fixed or parametric activation functions whilst a single hidden layer is used for KAF activation function. $^{**}$: feedforward neural networks with 5 hidden layers and 300 neurons each for fixed or parametric activation functions whilst 2 hidden layers with the same number of neurons for non-parametric activation functions \\ $^{***}$: CNNs made by stacking 5 convolutional blocks, each composed by (a) a convolutive layer with 150 filters, with a filter size of 5 × 5 and a stride of 1; (b) a max-pooling operation over 3 × 3 windows with stride of 2; (c) a dropout layer with probability of 0.25. See the original article for a full description of the architectures, hyperparams and training settings.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Robustness of Kafnets}{33}{part.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Related Works}{35}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}K-Winners Take All}{35}{section.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces A plot of a 1-dimensional k-WTA interpolating a curve. As stressed by the figure, k-WTAs are piece-wise linar functions where points of non-differentiability are very much densely distributed. Any small change in the input results in an abrupt change in the function's value. Source: (Ref. )\relax }}{36}{figure.caption.21}\protected@file@percent }
\newlabel{fig:kwta}{{5.1}{36}{A plot of a 1-dimensional k-WTA interpolating a curve. As stressed by the figure, k-WTAs are piece-wise linar functions where points of non-differentiability are very much densely distributed. Any small change in the input results in an abrupt change in the function's value. Source: (Ref. )\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Smooth Adversarial Training}{36}{section.5.2}\protected@file@percent }
\newlabel{SAT}{{5.2}{36}{Smooth Adversarial Training}{section.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Plot of the first derivative for ReLU, ELU and Swish activations. Except for the rectified linear unit, both the ELU and Swish have continuous derivative.\relax }}{37}{figure.caption.22}\protected@file@percent }
\newlabel{fig:smoothaf}{{5.2}{37}{Plot of the first derivative for ReLU, ELU and Swish activations. Except for the rectified linear unit, both the ELU and Swish have continuous derivative.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Compared to ReLU, all smooth activation functions showed a significant increase in robustness while some even improved accuracy. In the original work, 2 others smooth activation functions were used, namely SmoothReLU and Softplus. We did not include them since they are not commonly used in practice. Source: (Ref. Adversarlial Smoothness)\relax }}{38}{figure.caption.23}\protected@file@percent }
\newlabel{fig:smoothresults}{{5.3}{38}{Compared to ReLU, all smooth activation functions showed a significant increase in robustness while some even improved accuracy. In the original work, 2 others smooth activation functions were used, namely SmoothReLU and Softplus. We did not include them since they are not commonly used in practice. Source: (Ref. Adversarlial Smoothness)\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Solution Approach}{39}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{swenv}{{6}{39}{Solution Approach}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}KAFs May Be Good Candidates}{39}{section.6.1}\protected@file@percent }
\newlabel{fig:actdist11}{{\caption@xref {fig:actdist11}{ on input line 1223}}{40}{KAFs May Be Good Candidates}{figure.caption.24}{}}
\newlabel{fig:actdist12}{{\caption@xref {fig:actdist12}{ on input line 1228}}{41}{KAFs May Be Good Candidates}{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Each histogram shows the distribution of the activation values for the first neuron of the first channel, for each layer. The distribution colored in violet represents the activations for the batch of perturbed samples while the light-green stands for the distribution of values the neuron gets when we feed the network with original samples. We also plot, when possible, the actual shape of the activation function acting on the neuron with a red line. The effects of the perturbation can be appreciated especially in the last layer where distributions differ the most.\relax }}{41}{figure.caption.26}\protected@file@percent }
\newlabel{fig:actdist13}{{6.1}{41}{Each histogram shows the distribution of the activation values for the first neuron of the first channel, for each layer. The distribution colored in violet represents the activations for the batch of perturbed samples while the light-green stands for the distribution of values the neuron gets when we feed the network with original samples. We also plot, when possible, the actual shape of the activation function acting on the neuron with a red line. The effects of the perturbation can be appreciated especially in the last layer where distributions differ the most.\relax }{figure.caption.26}{}}
\newlabel{fig:actdist21}{{\caption@xref {fig:actdist21}{ on input line 1251}}{42}{KAFs May Be Good Candidates}{figure.caption.27}{}}
\newlabel{fig:actdist22}{{\caption@xref {fig:actdist22}{ on input line 1256}}{42}{KAFs May Be Good Candidates}{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Each histogram shows the distribution of the mean value of the activations for a random layer's filter, in the case of a convolutive layer, or for every neurons in the layer, in the case of dense (last) layer. The effects of the perturbation are less visible when the net is equipped with KAF activation functions.\relax }}{43}{figure.caption.29}\protected@file@percent }
\newlabel{fig:actdist23}{{6.2}{43}{Each histogram shows the distribution of the mean value of the activations for a random layer's filter, in the case of a convolutive layer, or for every neurons in the layer, in the case of dense (last) layer. The effects of the perturbation are less visible when the net is equipped with KAF activation functions.\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Fast is Better than Free Adversarial Training}{44}{section.6.2}\protected@file@percent }
\newlabel{fbf}{{6.2}{44}{Fast is Better than Free Adversarial Training}{section.6.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Evaluation}{47}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Learning rate evolution during a one-cyclic triangular learning rate schedule. Here the minimum value is 0.00005 and maximum value is 0.0005.\relax }}{48}{figure.caption.33}\protected@file@percent }
\newlabel{fig:oclr}{{7.1}{48}{Learning rate evolution during a one-cyclic triangular learning rate schedule. Here the minimum value is 0.00005 and maximum value is 0.0005.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}VGG Inspired Architectures Results}{48}{section.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Loss evolution plot with respect to exponentially increasing learning rates. The vertical dot-dashed red line denote the learning rate value where the loss has the steepest decrease. While the minimum value matches with the smallest learning rate, the maximum one should be picked around $10^{-1}$.\relax }}{49}{figure.caption.34}\protected@file@percent }
\newlabel{fig:lrf}{{7.2}{49}{Loss evolution plot with respect to exponentially increasing learning rates. The vertical dot-dashed red line denote the learning rate value where the loss has the steepest decrease. While the minimum value matches with the smallest learning rate, the maximum one should be picked around $10^{-1}$.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Exploding Gradients with KafNets}{49}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}ResNet20 Inspired Architectures Results}{49}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Future Works}{51}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Conclusions}{53}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
