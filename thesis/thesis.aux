\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Intriguing Properties of Neural Networks}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Smooth Activation Functions and Robustness}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Structure of the Thesis}{2}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Fundamentals}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Deep Neural Networks}{3}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Definition}{3}{subsection.2.1.1}\protected@file@percent }
\newlabel{layer}{{2.2}{3}{Definition}{equation.2.1.2}{}}
\newlabel{step}{{2.4}{4}{Definition}{equation.2.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Graphic representation of a one layer NN also know as MLP(ref. )\relax }}{4}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{}{4}{section*.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A Feedforward Neural Network with 3 hidden layers(ref. Michael A. Nielsen, Neural Networks and Deep Learning, Determination Press', 2015)\relax }}{5}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:fnn}{{2.2}{5}{A Feedforward Neural Network with 3 hidden layers(ref. Michael A. Nielsen, Neural Networks and Deep Learning, Determination Press', 2015)\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Training}{5}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss Function}{5}{section*.5}\protected@file@percent }
\newlabel{erm}{{2.5}{5}{Loss Function}{equation.2.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Descent}{5}{section*.6}\protected@file@percent }
\newlabel{bp1}{{2.7}{6}{Gradient Descent}{equation.2.1.7}{}}
\newlabel{bp2}{{2.8}{6}{Gradient Descent}{equation.2.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A 1-D representation of 4 gradient descent steps\relax }}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:gd}{{2.3}{7}{A 1-D representation of 4 gradient descent steps\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Convex and Non-convex optimization landscapes\relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig:convnonconv}{{2.4}{8}{Convex and Non-convex optimization landscapes\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Visualization of global, local and saddle points. How can A reach the global minimum?\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:globlocsad}{{2.5}{8}{Visualization of global, local and saddle points. How can A reach the global minimum?\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Activation Functions}{9}{subsection.2.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Plot of the Sigmoid function and its first derivative.\relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sigmoid}{{2.6}{9}{Plot of the Sigmoid function and its first derivative.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces ELU, LeakyReLU($\alpha =0.1$) and Swish functions plotted together. It can be seen that they mostly differ for negative values whereas behaving very similar to ReLU for positive arguments.\relax }}{10}{figure.caption.11}\protected@file@percent }
\newlabel{fig:afuncs}{{2.7}{10}{ELU, LeakyReLU($\alpha =0.1$) and Swish functions plotted together. It can be seen that they mostly differ for negative values whereas behaving very similar to ReLU for positive arguments.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}CNNs: Convolutional Neural Networks}{11}{subsection.2.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A convolution that produces the first element of the activation map for the given filter.\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:conv}{{2.8}{12}{A convolution that produces the first element of the activation map for the given filter.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Max (left) and Average (right) pooling layer with 2x2 window size.\relax }}{12}{figure.caption.13}\protected@file@percent }
\newlabel{fig:pooling}{{2.9}{12}{Max (left) and Average (right) pooling layer with 2x2 window size.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces General Architecture of CNN for ImageNet (Ref. ).\relax }}{13}{figure.caption.14}\protected@file@percent }
\newlabel{fig:cnnarch}{{2.10}{13}{General Architecture of CNN for ImageNet (Ref. ).\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}From Neural Networks to Deep Neural Networks}{13}{subsection.2.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Inception Block overview, $1\times 1$ convolutions are used to reduce the number of filters lowering complexity. Source: Dive into Deep Learning, chapt. 7.4\relax }}{14}{figure.caption.15}\protected@file@percent }
\newlabel{fig:inception}{{2.11}{14}{Inception Block overview, $1\times 1$ convolutions are used to reduce the number of filters lowering complexity. Source: Dive into Deep Learning, chapt. 7.4\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Source: Deep Residual Learning for Image Recognition\relax }}{15}{figure.caption.16}\protected@file@percent }
\newlabel{fig:degradation}{{2.12}{15}{Source: Deep Residual Learning for Image Recognition\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Source: Deep Residual Learning for Image Recognition\relax }}{16}{figure.caption.17}\protected@file@percent }
\newlabel{fig:residual}{{2.13}{16}{Source: Deep Residual Learning for Image Recognition\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Source: Dive Deep into Deep Learning\relax }}{16}{figure.caption.18}\protected@file@percent }
\newlabel{fig:resnet}{{2.14}{16}{Source: Dive Deep into Deep Learning\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Adversarial Examples Theory}{16}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Another Optimization problem}{17}{subsection.2.2.1}\protected@file@percent }
\newlabel{attackobj}{{2.25}{18}{Another Optimization problem}{equation.2.2.25}{}}
\newlabel{linfnorm}{{2.26}{18}{Another Optimization problem}{equation.2.2.26}{}}
\newlabel{targetedattack}{{2.28}{18}{Another Optimization problem}{equation.2.2.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Fast Gradient Sign Method}{18}{subsection.2.2.2}\protected@file@percent }
\newlabel{pgd}{{2.29}{18}{Fast Gradient Sign Method}{equation.2.2.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Projected Gradient Descent}{19}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}White, Grey and Black Box Attacks}{20}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Defenses}{20}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Detection Methods}{21}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Metzen \textit  {et al.} (Ref. ) used a ResNet as original classifier, plus dections is made thanks to many interleaving detectors between each residual block. Each detector is implemented as a DNN that learns how to spot the presence of an attack by looking at the activations layer's distributions during training.\relax }}{21}{figure.caption.19}\protected@file@percent }
\newlabel{fig:detection}{{2.15}{21}{Metzen \textit {et al.} (Ref. ) used a ResNet as original classifier, plus dections is made thanks to many interleaving detectors between each residual block. Each detector is implemented as a DNN that learns how to spot the presence of an attack by looking at the activations layer's distributions during training.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Provable Robustness}{22}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Adversarial Training}{22}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Kernel Based Activation Functions}{22}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Works}{23}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}K-Winners Take All}{23}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Smooth Adversarial Training}{23}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Solution Approach}{25}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Lipschitz Constant Approach}{25}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Fast is Better than Free Adversarial Training}{25}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Evaluation}{27}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}VGG Inspired Architectures Results}{27}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Explofing Gradients with KafNets}{27}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}ResNet20 Inspired Architectures Results}{27}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Works}{29}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{31}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
